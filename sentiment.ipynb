{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(review):\n",
    "    data_train_Exclude_tags = re.sub(r'<[^<>]+>', \" \", review)      # Excluding the html tags\n",
    "    data_train_num = re.sub(r'[0-9]+', 'number', data_train_Exclude_tags)  # Converting numbers to \"NUMBER\"\n",
    "    data_train_lower = data_train_num.lower()              # Converting to lower case.\n",
    "    data_train_split = data_train_lower.split()            # Splitting into individual words.\n",
    "    stopWords = set(stopwords.words(\"english\") )\n",
    "    stopWords.remove(\"not\")         #removing the \"not\" form stop words set\n",
    "    meaningful_words = [w for w in data_train_split if not w in stopWords]     # Removing stop words.\n",
    "    \n",
    "    return( \" \".join( meaningful_words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Data\n",
    "data_train = pd.read_csv('labeledTrainData.tsv',delimiter = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "x = data_train['review']\n",
    "y = data_train['sentiment']\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,) (5000,) (20000,) (5000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23311    0\n",
       "23623    0\n",
       "1020     0\n",
       "12645    1\n",
       "1533     1\n",
       "        ..\n",
       "21575    0\n",
       "5390     1\n",
       "860      1\n",
       "15795    1\n",
       "23654    0\n",
       "Name: sentiment, Length: 20000, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of stop words!\n",
      "{'that', 'did', 'we', \"mightn't\", 'should', 'won', 'myself', 're', 'll', 'why', 'are', 'my', 's', \"didn't\", 'i', 'but', 'most', \"hadn't\", 'by', \"isn't\", 'some', 'because', 'mightn', 'does', 'the', 'yourself', 'while', 'few', \"should've\", 'under', 'he', 'same', 'has', 'again', 'her', 'further', 'each', \"shouldn't\", \"you'd\", \"don't\", 'themselves', 'had', 'such', 'm', 'after', 'to', 'than', 'own', 'an', 'more', 'our', 'with', 'here', 'until', 'for', \"shan't\", 'hers', \"needn't\", 'of', 'down', \"won't\", 'have', 'before', 'shouldn', 'their', 'at', 'there', 'both', 'below', 've', 'were', 'himself', \"she's\", 'herself', 'am', 'about', 'being', 'on', 'be', 'and', 'when', 'they', 'o', \"that'll\", 'who', 'now', 'your', 'between', 'those', \"aren't\", 'doing', 'don', 'against', 'needn', 'whom', 'wouldn', 'what', 'ourselves', 'is', 'during', 'wasn', \"you'll\", 'isn', 'from', 'over', 'where', 'no', 'd', \"doesn't\", 'shan', 'ain', \"wouldn't\", 'so', 'you', 'them', 'these', 'then', 'his', 'just', 'didn', \"wasn't\", 'yourselves', 'if', \"haven't\", 'me', 'very', 'doesn', 'him', 'other', 'was', 'couldn', 'only', \"you're\", 'do', 'through', 'aren', 'weren', \"couldn't\", 't', 'a', 'itself', 'its', 'once', 'not', \"mustn't\", \"you've\", 'too', 'having', 'or', 'it', 'off', 'hadn', 'haven', 'nor', \"weren't\", 'ma', 'yours', 'above', 'will', 'in', 'up', 'hasn', \"hasn't\", 'mustn', 'which', 'any', 'y', 'she', \"it's\", 'been', 'as', 'out', 'into', 'ours', 'theirs', 'all', 'can', 'how', 'this'}\n",
      "---Ended---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"List of stop words!\")\n",
    "a = set(stopwords.words(\"english\") )\n",
    "print(a)\n",
    "print(\"---Ended---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie plain dumb. casting ralph meeker mike hammer fatuous climax, film exercise wooden predictability. mike hammer one detective fiction's true sociopaths. unlike marlow spade, put pieces together solve mystery, hammer breaks things apart get truth. film turns hammer boob surrounding bad guys ... well, dumb get away anything. one poorly drawn succumbs popcorn attack. parts movie right three stooges play book. velda's dance barre, instance, bad guy accidentally stabs boss back. continuity breaks shameful: frau blucher running centerline road camera tight lower legs way side camera pulls back wider shot. worst break, however, precedes popcorn attack. bad guy stalking hammer passes clock seconds hero, except clock shows seven minutes behind guy. fair, interesting camera angles lighting, grand finale bad must seen, reason gets two points number.\n"
     ]
    }
   ],
   "source": [
    "# cleaning the data.\n",
    "cleanWords = []\n",
    "for i in range(x_train.size):\n",
    "    cleanWords.append(cleaning_data(x_train.iloc[i]))\n",
    "print(cleanWords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating features from bags of words.\n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000)\n",
    "data_train_features = vectorizer.fit_transform(cleanWords)\n",
    "data_train_features = data_train_features.toarray()         # 25000x5000 sparse matrix, with 2105457 stored elements in compressed Sparse Row format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the classifier\n",
      "\n",
      "accuracy:  0.86655\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "print(\"Training the classifier\\n\")\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "nb_model = BernoulliNB().fit(data_train_features, y_train)\n",
    "print(\"accuracy: \",nb_model.score(data_train_features, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Review Processing Done!---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testcleanWords = []\n",
    "for i in range(x_test.size):\n",
    "    #print('Processin', i)\n",
    "    testcleanWords.append( cleaning_data( x_test.iloc[i] ))\n",
    "print(\"---Review Processing Done!---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Features Created!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating features from bags of words.\n",
    "data_test_features = vectorizer.transform(testcleanWords)\n",
    "#data_train_features = data_train_features.toarray()         # 25000x5000 sparse matrix, with 2105457 stored elements in compressed Sparse Row format.\n",
    "print(\"Test Features Created!!!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2124  357]\n",
      " [ 363 2156]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86      2481\n",
      "           1       0.86      0.86      0.86      2519\n",
      "\n",
      "    accuracy                           0.86      5000\n",
      "   macro avg       0.86      0.86      0.86      5000\n",
      "weighted avg       0.86      0.86      0.86      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "predicted = nb_model.predict(data_test_features)\n",
    "matrix = confusion_matrix(y_test, predicted)\n",
    "print(matrix)\n",
    "report = classification_report(y_test, predicted)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
